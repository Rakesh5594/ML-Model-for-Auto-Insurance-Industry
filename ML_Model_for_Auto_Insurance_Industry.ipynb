{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Aim of the Project**\n",
        "\n",
        "The aim of the project is to build a Machine Learning Model to predict whether an owner will initiate an auto insurance claim in the next year.\n",
        "\n",
        "**Background**\n",
        "\n",
        "The auto insurance industry is witnessing a paradigm shift. Since auto insurance company consists of homogenous good thereby making it difficult to differentiate product A from product B, also companies are fighting a price war (for insurance price). On top of that, the distribution channel is shifting more from traditional insurance brokers to online purchases, which means that the ability for companies to interact through human touchpoints is limited, and customers should be quoted at a reasonable price. A good price quote is one that makes the customer purchase the policy and helps the company to increase the profits. Also, the insurance premium is calculated based on more than 50+ parameters, which means that traditional business analytics-based algorithms are now limited in their ability to differentiate among customers based on subtle parameters.\n",
        "\n",
        "**The model shall mainly support the following use cases:**\n",
        "\n",
        "1. Conquering Market Share: Capture market share by lowering the prices of the premium for the customers, who are least likely to claim.\n",
        "\n",
        "2. Risk Management: Charge the right premium from the customer, who is likely to claim insurance in the coming year\n",
        "\n",
        "3. Smooth Processing: Reduce the complexity of pricing models. Most of the transactions are happening online with larger customer attributes (thanks to the internet and social media). Harness the power of huge data to build complex ML models\n",
        "\n",
        "4. Increased Profits: As per industry estimate 1% reduction in the claim can boost profit by 10%. So, through the ML model, we can identify and deny the insurance to the driver who will make a claim. Thus, ensuring reduced claim outgo and increased profit.\n",
        "\n",
        "Part of the model development is to identify and prioritize the above use cases."
      ],
      "metadata": {
        "id": "rftgRSj0zCh3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LXUOZolqy3lh"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix,mean_squared_error,accuracy_score\n",
        "from sklearn.metrics import precision_score,recall_score,roc_auc_score,f1_score,cohen_kappa_score\n",
        "from sklearn.utils import resample\n",
        "from pprint import pprint\n",
        "\n",
        "import plotly as py\n",
        "import plotly.graph_objs as go\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "from IPython.display import Markdown, display\n",
        "import statsmodels.api as sm # import API\n",
        "from matplotlib.pyplot import xticks\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "pd.set_option('display.max_columns', 100)\n",
        "py.offline.init_notebook_mode(connected=True)\n",
        "pd.options.display.float_format = '{:20,.2f}'.format\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "QSeQAENJzwaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Helper Utilities"
      ],
      "metadata": {
        "id": "t-OKDjkSz3Sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log(string):\n",
        "    display(Markdown(\"> <span style='color:blue'>\"+string+\"</span>\"))\n",
        "\n",
        "def header(string):\n",
        "    display(Markdown(\"------ \"))\n",
        "    display(Markdown(\"### \"+string))\n",
        "\n",
        "def get_variable_type(element) :\n",
        "    \"\"\"\n",
        "     Check is columns are of Continuous or Categorical variable.\n",
        "     Assumption is that if\n",
        "                 unique count < 20 then categorical\n",
        "                 unique count >= 20 and dtype = [int64 or float64] then continuous\n",
        "     \"\"\"\n",
        "    if element==0:\n",
        "        return \"Not Known\"\n",
        "    elif element < 20 and element!=0 :\n",
        "        return \"Categorical\"\n",
        "    elif element >= 20 and element!=0 :\n",
        "        return \"Contineous\"\n",
        "\n",
        "def predict_variable_type(metadata_matrix):\n",
        "    metadata_matrix[\"Variable_Type\"] = metadata_matrix[\"Unique_Values_Count\"].apply(get_variable_type).astype(str)\n",
        "    metadata_matrix[\"frequency\"] = metadata_matrix[\"Null_Count\"] - metadata_matrix[\"Null_Count\"]\n",
        "    metadata_matrix[\"frequency\"].astype(int)\n",
        "    return metadata_matrix\n",
        "\n",
        "def get_meta_data(dataframe) :\n",
        "    \"\"\"\n",
        "     Method to get Meta-Data about any dataframe passed\n",
        "    \"\"\"\n",
        "    metadata_matrix = pd.DataFrame({\n",
        "                    'Datatype' : dataframe.dtypes.astype(str), # data types of columns\n",
        "                    'Non_Null_Count': dataframe.count(axis = 0).astype(int), # total elements in columns\n",
        "                    'Null_Count': dataframe.isnull().sum().astype(int), # total null values in columns\n",
        "                    'Null_Percentage': dataframe.isnull().sum()/len(dataframe) * 100, # percentage of null values\n",
        "                    'Unique_Values_Count': dataframe.nunique().astype(int) # number of unique values\n",
        "                     })\n",
        "\n",
        "    metadata_matrix = predict_variable_type(metadata_matrix)\n",
        "    return metadata_matrix\n",
        "\n",
        "def plot_data_pie_chat(dataframe,col) :\n",
        "    header(\"Stats for \"+col+\" Datatype Percentage Distribution\")\n",
        "    dataframe_group = dataframe.groupby(col).frequency.count().reset_index()\n",
        "    dataframe_group.sort_values([col], axis=0,ascending=False, inplace=True)\n",
        "    trace = go.Pie(labels=dataframe_group[col].tolist(), values=dataframe_group[\"frequency\"].tolist())\n",
        "    layout = go.Layout(title=\"Datatype Percentage Distribution\")\n",
        "    fig = go.Figure(data=[trace], layout=layout)\n",
        "    py.offline.iplot(fig)\n",
        "\n",
        "def pairplot(x_axis,y_axis) :\n",
        "    sns.pairplot(car_df,x_vars=x_axis,y_vars=y_axis,height=4,aspect=1,kind=\"scatter\")\n",
        "    plt.show()\n",
        "\n",
        "def heatmap(x,y,dataframe):\n",
        "    plt.figure(figsize=(x,y))\n",
        "    sns.heatmap(dataframe.corr(),cmap=\"OrRd\",annot=True)\n",
        "    plt.show()\n",
        "\n",
        "def bar_count_plot(dataframe,col_name) :\n",
        "    plt.figure(figsize=(16,8))\n",
        "    plt.title(col_name + 'Histogram')\n",
        "    sns.countplot(dataframe[col_name], palette=(\"plasma\"))\n",
        "    xticks(rotation = 90)\n",
        "    plt.show()\n",
        "\n",
        "def color_red(val):\n",
        "    \"\"\"\n",
        "    Takes a scalar and returns a string with\n",
        "    the css property `'color: red'` for value\n",
        "    greater than 10 , black otherwise.\n",
        "    \"\"\"\n",
        "    color = 'red' if val > 5 else 'black'\n",
        "    return 'color: %s' % color\n",
        "\n",
        "def accuracy_result(y_test, y_pred_test):\n",
        "    from sklearn import metrics\n",
        "    from sklearn.metrics import confusion_matrix, classification_report\n",
        "    confusion_matrix=metrics.confusion_matrix(y_test, y_pred_test)\n",
        "    # USE THE IMPORTED CONFUSION MATRIX\n",
        "    print('\\n CONFUSION MATRIX:\\n ', confusion_matrix,'\\n')\n",
        "    TP = confusion_matrix[1, 1]\n",
        "    TN = confusion_matrix[0, 0]\n",
        "    FP = confusion_matrix[0, 1]\n",
        "    FN = confusion_matrix[1, 0]\n",
        "    false_positive_rate = round(FP / float(TN + FP),3)\n",
        "    print('FPR: ', false_positive_rate)\n",
        "    print('TPR/ RECALL/ SENSTIVITY: ', round(metrics.recall_score(y_test, y_pred_test), 3))\n",
        "    print('PRECISION:' ,round(metrics.precision_score(y_test, y_pred_test), 3))\n",
        "    specificity = round(TN / (TN + FP),3)\n",
        "    print('SPECIFICITY: ',specificity)\n",
        "    print('ACCURACY: ', np.round(metrics.accuracy_score(y_test, y_pred_test),3))\n",
        "    print('ROC AUC: ', np.round(roc_auc_score(y_test, y_pred_test),3))\n",
        "    print('Cohens kappa: ',np.round(cohen_kappa_score(y_test, y_pred_test),3))\n",
        "    print('F1 score: ', np.round(f1_score(y_test, y_pred_test),3))\n",
        "    print('\\n CLASSIFICATION REPORT: \\n',classification_report(y_test,y_pred_test))\n",
        "    return"
      ],
      "metadata": {
        "id": "Vp8wd36Dz8el"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import data"
      ],
      "metadata": {
        "id": "rSUjO1kx0DTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/kaggle/input/train.csv')\n",
        "df1= df.copy()\n",
        "df.head()"
      ],
      "metadata": {
        "id": "p1C_aa-20EVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = df[:200000]\n",
        "_X = temp.drop(['id','target'],axis =1)\n",
        "_y=temp.target\n",
        "metadata_matrix_dataframe = get_meta_data(df)\n",
        "metadata_matrix_dataframe[\"dt_name\"] =[ i.split(\"_\")[-1] for i in metadata_matrix_dataframe.index.values]\n",
        "metadata_matrix_dataframe['dt_name'] = metadata_matrix_dataframe['dt_name'].apply(lambda x : \"interval\" if str(x).isnumeric() else x)"
      ],
      "metadata": {
        "id": "Yw04s8fj0IML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dtype = metadata_matrix_dataframe.groupby(['dt_name'])\n",
        "\n",
        "interval_feature =Dtype.get_group(\"interval\").index.tolist()\n",
        "bin_feature = Dtype.get_group(\"bin\").index.tolist()\n",
        "cat_feature = Dtype.get_group(\"cat\").index.tolist()"
      ],
      "metadata": {
        "id": "t5eg8E7L0JJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write at least 3 important inferences from the data above\n",
        "\n",
        "  Ans.:\n",
        "  1. Data is higly imblaced, target distribtion is:\n",
        "\n",
        "        0 = 96.36 %\n",
        "\n",
        "        1 = 3.64 %\n",
        "\n",
        "  2. data have vaiable type:\n",
        "\n",
        "        Interval = 45.6\n",
        "\n",
        "        binary =29.8\n",
        "\n",
        "        categorical = 24.6\n",
        "\n",
        "  3. Most of the features are uncorrelated"
      ],
      "metadata": {
        "id": "tOJg9pE_0PoV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Is the data balanced? Meaning are targets 0 and 1 in the right proportion?\n",
        "\n",
        "  Ans.: No, data not balanced. I shown below."
      ],
      "metadata": {
        "id": "PE7aaoTB0waI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perc = (df['target'].value_counts()/(df.shape[0]))*100\n",
        "print(\"Percentge distribution of class '1' & class '0':\\n\\n\",perc)\n",
        "perc.plot.bar();"
      ],
      "metadata": {
        "id": "NYS4ulAo0zeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. How many categorical features are there?\n"
      ],
      "metadata": {
        "id": "j5H6jJUd019W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "group_variable_type = metadata_matrix_dataframe.groupby('Variable_Type')\n",
        "print(\"categorical features count:\",group_variable_type.get_group('Categorical').shape[0])"
      ],
      "metadata": {
        "id": "ISUqewzQ04qE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Write inferences from data on interval variables"
      ],
      "metadata": {
        "id": "cEOh53Pi06XT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp = df[interval_feature].corr()\n",
        "temp = temp[(temp>=0.5) | (temp<=-0.5)].fillna(0)\n",
        "fig, ax = plt.subplots(figsize=(12,10))\n",
        "sns.heatmap(temp,linewidths=.5, cmap=\"YlGnBu\",ax=ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nCLHTKa60_zZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log(\"Analysis : Almost all the features are independent, Looking at above plot only 3 features are correlated with correlation near 0.5.\")"
      ],
      "metadata": {
        "id": "wYppBCIL1D6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write inferences from data on ordinal variables."
      ],
      "metadata": {
        "id": "1zSYPTrz1G9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d ={}\n",
        "for i in df[cat_feature].columns:\n",
        "    d[i] = df[cat_feature][i].nunique()\n",
        "\n",
        "pd.DataFrame(d,index=['unique values']).T.plot.bar(figsize=(16,8));"
      ],
      "metadata": {
        "id": "8OYSsAfp1IK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log(\"Analysis : Looking at above graph only one feature have unqiue values more than 20\")\n"
      ],
      "metadata": {
        "id": "5nN4EQtg1KCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write inferences from data on binary variables."
      ],
      "metadata": {
        "id": "PhsDsS-_1L7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col = metadata_matrix_dataframe[metadata_matrix_dataframe['Unique_Values_Count'] == 2].index\n",
        "df_1 = pd.DataFrame((df[col].sum()/df[col].shape[0])*100, columns=['Percentage of 1'])\n",
        "df_1.plot.bar(figsize=(16,8));\n",
        "log(\"Analysis : Looking at above graph % of 1's is near near for *ps_ind_10_bin, ps_ind_11_bin, ps_ind_12_bin, ps_ind_13_bin.\");"
      ],
      "metadata": {
        "id": "ZHAJCUNj1PWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Check if the target data is proportionate or not. Hint: Below than 30% for binary data is sign of imbalance"
      ],
      "metadata": {
        "id": "maq89Wp41dpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Per1= np.round((df[df['target']==1].shape[0]*100)/ df.shape[0],2)\n",
        "Per0= np.round((df[df['target']==0].shape[0]*100)/ df.shape[0],2)\n",
        "\n",
        "print('Target=1 shape:',df[df['target']==1].shape, Per1,'%','\\nTarget=0 shape:',df[df['target']==0].shape,Per0,'%')\n",
        "print(\"\\n # This Data shows that it's imbalanced\")\n",
        "sns.countplot(x= df['target'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oXDKtnWa1fQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What should be the preferred way in this case to balance the data?\n",
        "\n",
        "  The most popular solution to an imbalanced classification problem is to change the composition of the training dataset.Techniques designed to change the class distribution in the training dataset are generally referred to as sampling methods or resampling methods as we are sampling an existing data sample."
      ],
      "metadata": {
        "id": "EMWU1Am61hza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How many training records are there after achieving a balance of 12%?"
      ],
      "metadata": {
        "id": "YzK7NIdt1l5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using resampling technique, splittinf data into classes\n",
        "df_mino= df[df['target']==1]\n",
        "df_majo= df[df['target']==0]\n",
        "df_mino_upsampled = resample(df_mino,replace=True,n_samples=int(len(df_majo)*0.12), random_state = 42)\n",
        "\n",
        "print(\"Size of data, after achieving a balance of 12%: \", df_mino_upsampled.shape[0] + df_majo.shape[0])"
      ],
      "metadata": {
        "id": "v3MCRSk01pbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying Resampling to make balance data-set"
      ],
      "metadata": {
        "id": "kACzJXCa1rbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_mino_upsampled = resample(df_mino,replace=True,n_samples=len(df_majo), random_state = 42)\n",
        "df = pd.concat([df_majo,df_mino_upsampled])\n",
        "print(\"Class compostion after resampling:\")\n",
        "print(df['target'].value_counts())\n",
        "df['target'].value_counts().plot.bar();"
      ],
      "metadata": {
        "id": "RjQxEcce1qG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now here we have both class with equall count\n"
      ],
      "metadata": {
        "id": "Jvcko-3l1wCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Which are the top two features in terms of missing values?\n",
        "  \n",
        "  No Missing Value Found.\n",
        "\n",
        "12. In total, how many features have missing values?\n",
        "\n",
        "  No Feature having missing values\n",
        "\n",
        "13. What steps should be taken to handle the missing data?\n",
        "\n",
        "  NA"
      ],
      "metadata": {
        "id": "uIdILOiP1TPO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Which interval variables have strong correlation?"
      ],
      "metadata": {
        "id": "RNKtMQVg1y3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp = df[interval_feature].corr()\n",
        "temp = temp[(temp>=0.5) | (temp<=-0.5)].fillna(0)\n",
        "fig, ax = plt.subplots(figsize=(12,10))\n",
        "sns.heatmap(temp,linewidths=.5, cmap=\"YlGnBu\",ax=ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lmYD9Kx7109o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "interval Variable with high correlation(abs value more than 0.5)\n",
        "\n",
        "ps_reg_03 --> ps_reg_03 & ps_reg_03\n",
        "\n",
        "ps_reg_13 --> ps_reg_12"
      ],
      "metadata": {
        "id": "75dbXc-A12dU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What's the level of correlation among ordinal features?"
      ],
      "metadata": {
        "id": "RzoHP3by18fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cutoff = 0.3\n",
        "temp = df[cat_feature].corr()\n",
        "temp = temp[(temp>=0.3) | (temp<=-0.3)].fillna(0)\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "sns.heatmap(temp,linewidths=.3, cmap=\"YlGnBu\",ax=ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bsxzUCUZ1-T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ordinal Variable with high correlation(abs value more than 0.5)\n",
        "\n",
        "ps_car_03_cat --> ps_car_05_cat\n",
        "\n",
        "ps_car_05_cat --> ps_car_09_cat"
      ],
      "metadata": {
        "id": "0b3LBdmC2Ct3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Implement Hot Encoding for categorical features\n",
        "\n",
        "    NA\n",
        "\n",
        "  No, nominal Features are available"
      ],
      "metadata": {
        "id": "Tx2TZUzm2GqA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. In nominal and interval features, which features are suitable for StandardScaler?\n",
        "\n",
        "  Nominal featues are not suitable for StandardScaler\n",
        "\n",
        "  We can apply StandardScaler in interval features"
      ],
      "metadata": {
        "id": "pERzGUzq2JhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf=RandomForestClassifier(n_estimators=100)\n",
        "clf.fit(_X,_y)\n",
        "\n",
        "feature_imp = pd.Series(clf.feature_importances_,index=_X.columns).sort_values(ascending=False)\n",
        "imp = pd.DataFrame(feature_imp).reset_index()\n",
        "imp.columns = [\"Features\", 'Imp']\n",
        "\n",
        "imp['Imp'] = (imp['Imp']/max(imp['Imp']))*100\n",
        "col = imp[imp[\"Imp\"]>25]['Features'].tolist()"
      ],
      "metadata": {
        "id": "Y1AkZLcC2OPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a bar plot\n",
        "plt.figure(figsize = (20,7))\n",
        "sns.barplot(x=_X.columns, y=imp.Imp)\n",
        "plt.xlabel('Feature Importance Score')\n",
        "plt.ylabel('Features')\n",
        "plt.title(\"Visualizing Important Features\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KwFRGASz2Rx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Top 10 ordered important features\")\n",
        "print(imp[:10].Features);"
      ],
      "metadata": {
        "id": "p4YXUQ0W2Tng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Summarize the learnings of ED\n",
        "\n",
        "Data is higly imblaced, target distribtion is:\n",
        "\n",
        "    0 = 96.36 %\n",
        "    1 = 3.64 %\n",
        "\n",
        "2 Most of the features are uncorrelated\n",
        "\n",
        "Top 10 ordered important features\n",
        "\n",
        "    ps_car_13\n",
        "    ps_reg_03\n",
        "    ps_car_14\n",
        "    ps_calc_10\n",
        "    ps_calc_14\n",
        "    ps_calc_11\n",
        "    ps_car_11_cat\n",
        "    ps_ind_15\n",
        "    ps_ind_03\n",
        "    ps_calc_01"
      ],
      "metadata": {
        "id": "0UfXfGfG2WhK"
      }
    }
  ]
}